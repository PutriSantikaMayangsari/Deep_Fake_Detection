{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127e8f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sakspari/Documents/magister/CVL/DeepFakeDetection/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "# --- new dependencies ---\n",
    "from facenet_pytorch import MTCNN\n",
    "from skimage.feature import local_binary_pattern\n",
    "import timm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# --- configuration ---\n",
    "CONFIG = {\n",
    "    \"real_data_path\": \"./datasets/ff-c23/FaceForensics++_C23/original\",\n",
    "    \"fake_data_path\": \"./datasets/ff-c23/FaceForensics++_C23/Deepfakes\",\n",
    "    \"max_videos_per_class\": 1000,\n",
    "    \"frames_per_video\": 20,\n",
    "    \"image_size\": 224,\n",
    "    \"batch_size\": 32,\n",
    "    \"epochs\": 30,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"seed\": 42,\n",
    "    \"model_name\": \"xception\",\n",
    "    \"model_save_path\": \"./model_checkpoints_lbp\",\n",
    "    \"early_stopping_patience\": 5,\n",
    "    \"weight_decay\": 1e-5,\n",
    "}\n",
    "\n",
    "torch.manual_seed(CONFIG[\"seed\"])\n",
    "np.random.seed(CONFIG[\"seed\"])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- step 1: data preparation ---\n",
    "mtcnn = MTCNN(keep_all=False, post_process=False, min_face_size=20, device=device)\n",
    "\n",
    "def apply_gaussian_and_lbp(image):\n",
    "    img_np_gray = np.array(image.convert('L'))\n",
    "    img_blur = cv2.GaussianBlur(img_np_gray, (3, 3), 0)\n",
    "    n_points = 24\n",
    "    radius = 3\n",
    "    lbp = local_binary_pattern(img_blur, n_points, radius, method='uniform')\n",
    "    lbp = (lbp / np.max(lbp) * 255).astype(np.uint8)\n",
    "    lbp_rgb = cv2.cvtColor(lbp, cv2.COLOR_GRAY2RGB)\n",
    "    return Image.fromarray(lbp_rgb)\n",
    "\n",
    "def prepare_frames_dataset(config):\n",
    "    print(\"preparing dataset of lbp processed frames...\")\n",
    "    all_images = []\n",
    "    all_labels = []\n",
    "\n",
    "    def process_videos(video_paths, label, desc):\n",
    "        for video_path in tqdm(video_paths, desc=desc):\n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            if not cap.isOpened():\n",
    "                continue\n",
    "            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            frame_indices = np.linspace(0, frame_count - 1, config[\"frames_per_video\"], dtype=int)\n",
    "            for i in frame_indices:\n",
    "                cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    continue\n",
    "                frame_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "                face_tensor = mtcnn(frame_pil)\n",
    "                if face_tensor is not None:\n",
    "                    face_pil = transforms.ToPILImage()(face_tensor)\n",
    "                    lbp_image = apply_gaussian_and_lbp(face_pil)\n",
    "                    all_images.append(lbp_image)\n",
    "                    all_labels.append(label)\n",
    "            cap.release()\n",
    "\n",
    "    real_videos = [os.path.join(config[\"real_data_path\"], f) for f in os.listdir(config[\"real_data_path\"])][:config[\"max_videos_per_class\"]]\n",
    "    process_videos(real_videos, 0, \"processing real videos\")\n",
    "    fake_videos = [os.path.join(config[\"fake_data_path\"], f) for f in os.listdir(config[\"fake_data_path\"])][:config[\"max_videos_per_class\"]]\n",
    "    process_videos(fake_videos, 1, \"processing fake videos\")\n",
    "\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "        all_images, all_labels, test_size=0.20, random_state=config[\"seed\"], stratify=all_labels\n",
    "    )\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_val, y_train_val, test_size=0.20, random_state=config[\"seed\"], stratify=y_train_val\n",
    "    )\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "# --- dataset ---\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        return self.transform(image), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# --- spatiotemporal attention ---\n",
    "class SpatiotemporalAttention(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super(SpatiotemporalAttention, self).__init__()\n",
    "        self.conv1x1 = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
    "        self.sa_conv1 = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
    "        self.layer_norm = nn.LayerNorm(in_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sa_conv2 = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
    "        self.ta_q = nn.Linear(in_channels, in_channels)\n",
    "        self.ta_k = nn.Linear(in_channels, in_channels)\n",
    "        self.ta_v = nn.Linear(in_channels, in_channels)\n",
    "    def forward(self, x):\n",
    "        b, t, c, h, w = x.shape\n",
    "        x_reshaped = x.view(b*t, c, h, w)\n",
    "        am = self.conv1x1(x_reshaped)\n",
    "        gf_intermediate = self.sa_conv1(am * x_reshaped)\n",
    "        gf_intermediate = gf_intermediate.view(b*t, c, -1).transpose(1,2)\n",
    "        gf_intermediate = self.layer_norm(gf_intermediate)\n",
    "        gf_intermediate = gf_intermediate.transpose(1,2).view(b*t, c, h, w)\n",
    "        gf = self.sa_conv2(self.relu(gf_intermediate))\n",
    "        saf = x_reshaped + gf\n",
    "        gf_pooled = F.adaptive_avg_pool2d(gf, (1,1)).view(b, t, c)\n",
    "        q = self.ta_q(gf_pooled)\n",
    "        k = self.ta_k(gf_pooled)\n",
    "        v = self.ta_v(gf_pooled)\n",
    "        attn = torch.bmm(q, k.transpose(1,2)) / (c ** 0.5)\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        tf_matrix = torch.bmm(attn, v)\n",
    "        tf = tf_matrix.view(b*t, c, 1, 1).expand(-1, -1, h, w)\n",
    "        staf = saf * tf + x_reshaped\n",
    "        return staf.view(b, t, c, h, w)\n",
    "\n",
    "# --- convlstm cell ---\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size=3):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.conv = nn.Conv2d(input_dim + hidden_dim, 4 * hidden_dim, kernel_size, padding=padding)\n",
    "    def forward(self, x, states):\n",
    "        h, c = states\n",
    "        combined = torch.cat([x, h], dim=1)\n",
    "        conv_out = self.conv(combined)\n",
    "        i, f, o, g = torch.split(conv_out, self.hidden_dim, dim=1)\n",
    "        i = torch.sigmoid(i)\n",
    "        f = torch.sigmoid(f)\n",
    "        o = torch.sigmoid(o)\n",
    "        g = torch.tanh(g)\n",
    "        c_next = f * c + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "        return h_next, c_next\n",
    "    def init_hidden(self, batch_size, spatial_size, device):\n",
    "        h, w = spatial_size\n",
    "        return (torch.zeros(batch_size, self.hidden_dim, h, w, device=device),\n",
    "                torch.zeros(batch_size, self.hidden_dim, h, w, device=device))\n",
    "\n",
    "# --- model sta + xception + convlstm ---\n",
    "class STA_Xception_ConvLSTM(nn.Module):\n",
    "    def __init__(self, model_name=\"xception\", num_classes=2, hidden_dim=256):\n",
    "        super(STA_Xception_ConvLSTM, self).__init__()\n",
    "        self.sta = SpatiotemporalAttention(in_channels=3)\n",
    "        base_model = timm.create_model(model_name, pretrained=True, num_classes=0, global_pool=\"\")\n",
    "        self.backbone = nn.Sequential(*list(base_model.children())[:-1])\n",
    "        feature_dim = base_model.num_features\n",
    "        self.convlstm = ConvLSTMCell(input_dim=feature_dim, hidden_dim=hidden_dim, kernel_size=3)\n",
    "        self.gap = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 4:\n",
    "            x = x.unsqueeze(1)\n",
    "        b, t, c, h, w = x.shape\n",
    "        x = self.sta(x)\n",
    "        h_state, c_state = None, None\n",
    "        for i in range(t):\n",
    "            frame_feat = self.backbone(x[:, i])\n",
    "            if frame_feat.dim() == 2:\n",
    "                frame_feat = frame_feat.unsqueeze(-1).unsqueeze(-1)\n",
    "            if h_state is None:\n",
    "                h_state, c_state = self.convlstm.init_hidden(b, (frame_feat.shape[2], frame_feat.shape[3]), frame_feat.device)\n",
    "            h_state, c_state = self.convlstm(frame_feat, (h_state, c_state))\n",
    "        out = self.gap(h_state).view(b, -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "def get_model(model_name, num_classes=2):\n",
    "    print(\"initializing model with sta and convlstm\")\n",
    "    return STA_Xception_ConvLSTM(model_name=model_name, num_classes=num_classes)\n",
    "\n",
    "# --- evaluation and plotting ---\n",
    "def plot_and_save_metrics(train_loss, val_loss, train_acc, val_acc, save_path=\"training_metrics_lbp.png\"):\n",
    "    epochs_range = range(1, len(train_loss) + 1)\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, train_loss, 'o-', label='training loss')\n",
    "    plt.plot(epochs_range, val_loss, 'o-', label='validation loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.grid(True)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, train_acc, 'o-', label='training accuracy')\n",
    "    plt.plot(epochs_range, val_acc, 'o-', label='validation accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.grid(True)\n",
    "    plt.suptitle('model training metrics lbp method')\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.savefig(save_path)\n",
    "    plt.show()\n",
    "    print(f\"metrics plot saved to {save_path}\")\n",
    "\n",
    "def evaluate_on_test_set(model, test_loader, device):\n",
    "    model.eval()\n",
    "    all_labels, all_preds, all_probs = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader, desc=\"evaluating on test set\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            if images.dim() == 4:\n",
    "                images = images.unsqueeze(1)\n",
    "            outputs = model(images)\n",
    "            probs = torch.softmax(outputs, dim=1)[:, 1]\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    print(\"\\n--- test set evaluation ---\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=['real', 'fake']))\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    fpr, tpr, _ = roc_curve(all_labels, all_probs)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'roc curve (area = {roc_auc:0.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['real', 'fake'])\n",
    "    disp.plot(cmap=plt.cm.Blues, ax=plt.gca())\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"test_evaluation_results_lbp.png\")\n",
    "    plt.show()\n",
    "    print(\"test evaluation plots saved to test_evaluation_results_lbp.png\")\n",
    "\n",
    "# --- main execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = prepare_frames_dataset(CONFIG)\n",
    "    print(f\"dataset prepared. train: {len(X_train)}, val: {len(X_val)}, test: {len(X_test)}\")\n",
    "\n",
    "    data_transforms = transforms.Compose([\n",
    "        transforms.Resize((CONFIG[\"image_size\"], CONFIG[\"image_size\"])),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "    ])\n",
    "\n",
    "    train_dataset = ImageDataset(X_train, y_train, data_transforms)\n",
    "    val_dataset = ImageDataset(X_val, y_val, data_transforms)\n",
    "    test_dataset = ImageDataset(X_test, y_test, data_transforms)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=True, num_workers=2, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=False, num_workers=2, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "    model = get_model(CONFIG[\"model_name\"]).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=CONFIG[\"learning_rate\"], weight_decay=CONFIG[\"weight_decay\"])\n",
    "\n",
    "    model_save_dir = CONFIG[\"model_save_path\"]\n",
    "    os.makedirs(model_save_dir, exist_ok=True)\n",
    "    best_model_path = os.path.join(model_save_dir, f\"best_model_{CONFIG['model_name']}.pth\")\n",
    "\n",
    "    train_loss_history, val_loss_history, train_acc_history, val_acc_history = [], [], [], []\n",
    "    best_val_accuracy = 0\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    print(f\"starting training for {CONFIG['model_name']} model with sta and convlstm...\")\n",
    "    for epoch in range(CONFIG[\"epochs\"]):\n",
    "        model.train()\n",
    "        running_loss, train_correct, train_total = 0.0, 0, 0\n",
    "        for images, labels in tqdm(train_loader, desc=f\"epoch {epoch+1}/{CONFIG['epochs']}\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            if images.dim() == 4:\n",
    "                images = images.unsqueeze(1)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_train_acc = 100 * train_correct / train_total\n",
    "        train_loss_history.append(epoch_loss)\n",
    "        train_acc_history.append(epoch_train_acc)\n",
    "\n",
    "        model.eval()\n",
    "        val_correct, val_total, running_val_loss = 0, 0, 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                if images.dim() == 4:\n",
    "                    images = images.unsqueeze(1)\n",
    "                outputs = model(images)\n",
    "                val_loss = criterion(outputs, labels)\n",
    "                running_val_loss += val_loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_val_acc = 100 * val_correct / val_total\n",
    "        epoch_val_loss = running_val_loss / len(val_loader)\n",
    "        val_acc_history.append(epoch_val_acc)\n",
    "        val_loss_history.append(epoch_val_loss)\n",
    "\n",
    "        print(f\"epoch {epoch+1}/{CONFIG['epochs']}, train loss: {epoch_loss:.4f}, train acc: {epoch_train_acc:.2f}%, val loss: {epoch_val_loss:.4f}, val acc: {epoch_val_acc:.2f}%\")\n",
    "\n",
    "        if epoch_val_acc > best_val_accuracy:\n",
    "            best_val_accuracy = epoch_val_acc\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            print(f\"validation accuracy improved, saving best model to {best_model_path}\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(f\"validation accuracy did not improve, counter: {epochs_no_improve}/{CONFIG['early_stopping_patience']}\")\n",
    "\n",
    "        if epochs_no_improve >= CONFIG['early_stopping_patience']:\n",
    "            print(\"early stopping triggered\")\n",
    "            break\n",
    "\n",
    "    print(\"\\n--- training finished ---\")\n",
    "    plot_and_save_metrics(train_loss_history, val_loss_history, train_acc_history, val_acc_history)\n",
    "\n",
    "    print(f\"\\nloading best model from {best_model_path} for final evaluation...\")\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "    evaluate_on_test_set(model, test_loader, device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
